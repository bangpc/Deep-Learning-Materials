{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    ":label:`chapter_linear_regression`\n",
    "\n",
    "\n",
    "To start off, we will introduce the problem of regression.  This is the task of predicting a *real valued target* $y$ given a data point $\\mathbf{x}$.  Regression problems are common in practice, arising whenever we want to predict a continuous numerical value.  Some examples of regression problems include predicting house prices, stock prices, length of stay (for patients in the hospital), tomorrow's temperature, demand forecasting (for retail sales), and many more.  Note that not every prediction problem is a regression problem.  In subsequent sections we will discuss classification problems, where our predictions are discrete categories.\n",
    "\n",
    "## Basic Elements of Linear Regression\n",
    "\n",
    "Linear regression, which dates to Gauss and Legendre, is perhaps the simplest, and by far the most popular approach to solving regression problems.  What makes linear regression *linear* is that we assume that the output truly can be expressed as a *linear* combination of the input features.\n",
    "\n",
    "\n",
    "### Linear Model\n",
    "\n",
    "To keep things simple, we will start with running example in which we consider the problem of estimating the price of a house (e.g. in dollars) based on area (e.g. in square feet) and age (e.g. in years).  More formally, the assumption of linearity suggests that our model can be expressed in the following form:\n",
    "\n",
    "$$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b$$\n",
    "\n",
    "In economics papers, it is common for authors to write out linear models in this format with a gigantic equation that spans multiple lines containing terms for every single feature.  For the high-dimensional data that we often address in machine learning, writing out the entire model can be tedious.  In these cases, we will find it more convenient to use linear algebra notation.  In the case of $d$ variables, we could express our prediction $\\hat{y}$ as follows:\n",
    "\n",
    "$$\\hat{y} = w_1 \\cdot x_1 + ... + w_d \\cdot x_d + b$$\n",
    "\n",
    "or alternatively, collecting all features into a single vector $\\mathbf{x}$ and all parameters into a vector $\\mathbf{w}$, we can express our linear model as \n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b.$$\n",
    "\n",
    ":eqlabel:`eq_linear_regression_single`\n",
    "\n",
    "\n",
    "Above, the vector $\\mathbf{x}$ corresponds to a single data point.  Commonly, we will want notation to refer to the entire dataset of all input data points.  This matrix, often denoted using a capital letter $X$, is called the *design matrix* and contains one row for every example, and one column for every feature.\n",
    "\n",
    "Given a collection of data points $X$ and a vector containing the corresponding target values $\\mathbf{y}$, the goal of linear regression is to find the *weight* vector $w$ and bias term $b$ (also called an *offset* or *intercept*) that associates each data point $\\mathbf{x}_i$ with an approximation $\\hat{y}_i$ of its corresponding label $y_i$.\n",
    "\n",
    "Expressed in terms of a single data point, this gives us the expression same as :eqref:`eq_linear_regression_single`. \n",
    "\n",
    "Finally, for a collection of data points $\\mathbf{X}$, the predictions $\\hat{\\mathbf{y}}$ can be expressed via the matrix-vector product:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf X \\mathbf{w} + b.$$\n",
    "\n",
    ":eqlabel:`eq_linear_regression`\n",
    "\n",
    "\n",
    "\n",
    "Even if we believe that the best model to relate $\\mathbf{x}$ and $y$ is linear, it's unlikely that we'd find data where $y$ lines up exactly as a linear function of $\\mathbf{x}$.  For example, both the target values $y$ and the features $X$ might be subject to some amount of measurement error.  Thus even when we believe that the linearity assumption holds, we will typically incorporate a noise term to account for such errors.\n",
    "\n",
    "Before we can go about solving for the best setting of the parameters $w$ and $b$, we will need two more things: (i) some way to measure the quality of the current model and (ii) some way to manipulate the model to improve its quality.\n",
    "\n",
    "### Training Data\n",
    "\n",
    "The first thing that we need is training data.  Sticking with our running example, we'll need some collection of examples for which we know both the actual selling price of each house as well as their corresponding area and age.  Our goal is to identify model parameters that minimize the error between the predicted price and the real price.  In the terminology of machine learning, the data set is called a *training data* or *training set*, a house (often a house and its price) here comprises one *sample*, and its actual selling price is called a *label*.  The two factors used to predict the label are called *features* or *covariates*.\n",
    "\n",
    "Typically, we will use $n$ to denote the number of samples in our dataset.\n",
    "We index the samples by $i$, denoting each input data point as $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$ and the corresponding label as $y^{(i)}$.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "In model training, we need to measure the error between the predicted value and the real value of the price.  Usually, we will choose a non-negative number as the error.  The smaller the value, the smaller the error.  A common choice is the square function.  For given parameters $\\mathbf{w}$ and $b$, we can express the error of our prediction on a given a sample as follows:\n",
    "\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,$$\n",
    "\n",
    "The constant $1/2$ is just for mathematical convenience, ensuring that after we take the derivative of the loss, the constant coefficient will be $1$.  The smaller the error, the closer the predicted price is to the actual price, and when the two are equal, the error will be zero.\n",
    "\n",
    "Since the training dataset is given to us, and thus out of our control, the error is only a function of the model parameters.  In machine learning, we call the function that measures the error the *loss function*.  The squared error function used here is commonly referred to as *square loss*.\n",
    "\n",
    "To make things a bit more concrete, consider the example below where we plot a regression problem for a one-dimensional case, e.g. for a model where house prices depend only on area.\n",
    "\n",
    "![Fit data with a linear model.](../img/fit_linreg.svg)\n",
    "\n",
    "As you can see, large differences between estimates $\\hat{y}^{(i)}$ and observations $y^{(i)}$ lead to even larger contributions in terms of the loss, due to the quadratic dependence. To measure the quality of a model on the entire dataset, we can simply average the losses on the training set.\n",
    "\n",
    "\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$) that minimize the average loss across all training samples:\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$\n",
    "\n",
    "\n",
    "### Analytic Solution\n",
    "\n",
    "Linear regression happens to be an unusually simple optimization problem.  Unlike nearly every other model that we will encounter in this book, linear regression can be solved easily with a simple formula, yielding a global optimum.  To start we can subsume the bias $b$ into the parameter $\\mathbf{w}$ by appending a column to the design matrix consisting of all $1s$.  Then our prediction problem is to minimize $||\\mathbf{y} - X\\mathbf{w}||$.  Because this expression has a quadratic form it is clearly convex, and so long as the problem is not degenerate (our features are linearly independent), it is strictly convex.\n",
    "\n",
    "Thus there is just one global critical point on the loss surface corresponding to the global minimum.  Taking the derivative of the loss with respect to $\\mathbf{w}$ and setting it equal to 0 gives the analytic solution:\n",
    "\n",
    "$$\\mathbf{w}^* = (\\mathbf X^T \\mathbf X)^{-1}\\mathbf X^T y$$\n",
    "\n",
    "While simple problems like linear regression may admit analytic solutions, you should not get used to such good fortune.  Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution confines one to an restrictive set of models that would exclude all of deep learning.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Even in cases where we cannot solve the models analytically, and even when the loss surfaces are high-dimensional and nonconvex, it turns out that we can still make progress.  Moreover, when those difficult-to-optimize models are sufficiently superior for the task at hand, figuring out how to train them is well worth the trouble.\n",
    "\n",
    "The key trick behind nearly all of deep learning and that we will repeatedly throughout this book is to reduce the error gradually by iteratively updating the parameters, each step moving the parameters in the direction that incrementally lowers the loss function.  This algorithm is called gradient descent.  On convex loss surfaces it will eventually converge to a global minimum, and while the same can't be said for nonconvex surfaces, it will at least lead towards a (hopefully good) local minimum.\n",
    "\n",
    "The most naive application of gradient descent consists of taking the derivative of the true loss, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow. We must pass over the entire dataset before making a single update.  Thus, we'll often settle for sampling a random mini-batch of examples every time we need to computer the update, a variant called *stochastic gradient descent*.\n",
    "\n",
    "In each iteration, we first randomly and uniformly sample a mini-batch $\\mathcal{B}$ consisting of a fixed number of training data examples.  We then compute the derivative (gradient) of the average loss on the mini batch with regard to the model parameters.  Finally, the product of this result and a predetermined step size $\\eta > 0$ are used to update the parameters in the direction that lowers the loss.\n",
    "\n",
    "We can express the update mathematically as follows ($\\partial$ denotes the partial derivative):\n",
    "\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)$$\n",
    "\n",
    "\n",
    "To summarize, steps of the algorithm are the following: (i) we initialize the values of the model parameters, typically at random; (ii) we iterate over the data many times, updating the parameters in each by moving the parameters in the direction of the negative gradient, as calculated on a random minibatch of data.\n",
    "\n",
    "\n",
    "For quadratic losses and linear functions we can write this out explicitly as follows. Note that $\\mathbf{w}$ and $\\mathbf{x}$ are vectors. Here the more elegant vector notation makes the math much more readable than expressing things in terms of coefficients, say $w_1, w_2, \\ldots w_d$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && =\n",
    "w - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\\n",
    "b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  && =\n",
    "b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the above equation $|\\mathcal{B}|$ represents the number of samples (batch size) in each mini-batch, $\\eta$ is referred to as *learning rate* and takes a positive number. It should be emphasized that the values of the batch size and learning rate are set somewhat manually and are typically not learned through model training. Therefore, they are referred to as *hyper-parameters*. What we usually call *tuning hyper-parameters* refers to the adjustment of these terms. In the worst case this is performed through repeated trial and error until the appropriate hyper-parameters are found. A better approach is to learn these as parts of model training. This is an advanced topic and we do not cover them here for the sake of simplicity.\n",
    "\n",
    "### Model Prediction\n",
    "\n",
    "After completing the training process, we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$ (in general the \"hat\" symbol denotes estimates).  Note that the parameters that we learn via gradient descent are not exactly equal to the true minimizers of the loss on the training set, that's because gradient descent converges slowly to a local minimum but does not achieve it exactly.  Moreover if the problem has multiple local minimum, we may not necessarily achieve the lowest minimum.  Fortunately, for deep neural networks, finding parameters that minimize the loss *on training data* is seldom a significant problem. The more formidable task is to find parameters that will achieve low loss on data that we have not seen before, a challenge called *generalization*. We return to these topics throughout the book.\n",
    "\n",
    "Given the learned linear regression model $\\hat{\\mathbf{w}}^\\top x + \\hat{b}$, we can now estimate the price of any house outside the training data set with area (square feet) as $x_1$ and house age (year) as $x_2$. Here, estimation also referred to as ‘model prediction’ or ‘model inference’.\n",
    "\n",
    "Note that calling this step *inference* is a misnomer, but has become standard jargon in deep learning.  In statistics, inference means estimating parameters and outcomes based on other data.  This misuse of terminology in deep learning can be a source of confusion when talking to statisticians.\n",
    "\n",
    "\n",
    "## From Linear Regression to Deep Networks\n",
    "\n",
    "So far we only talked about linear functions. While neural networks cover a much richer family of models, we can begin thinking of the linear model as a neural network by expressing it the language of neural networks. To begin, let's start by rewriting things in a 'layer' notation.\n",
    "\n",
    "### Neural Network Diagram\n",
    "\n",
    "Commonly, deep learning practitioners represent models visually using neural network diagrams. In :numref:`fig_single_neuron`, we represent linear regression with a neural network diagram. The diagram shows the connectivity among the inputs and output, but does not depict the weights or biases (which are given implicitly).\n",
    "\n",
    "![Linear regression is a single-layer neural network. ](../img/singleneuron.svg)\n",
    "\n",
    ":label:`fig_single_neuron`\n",
    "\n",
    "\n",
    "In the above network, the inputs are $x_1, x_2, \\ldots x_d$.  Sometimes the number of inputs are referred to as the feature dimension.  For linear regression models, we act upon $d$ inputs and output $1$ value.  Because there is just a single computed neuron (node) in the graph, we can think of linear models as neural networks consisting of just a single neuron. Since all inputs are connected to all outputs (in this case it's just one), this layer can also be regarded as an instance of a *fully-connected layer*, also commonly called a *dense layer*.\n",
    "\n",
    "### Biology\n",
    "\n",
    "Neural networks derive their name from their inspirations in neuroscience.  Although linear regression predates computation neuroscience, many of the models we subsequently discuss truly owe to neural inspiration.  To understand the neural inspiration for artificial neural networks it is worth while considering the basic structure of a neuron.  For the purpose of the analogy it is sufficient to consider the *dendrites* (input terminals), the *nucleus* (CPU), the *axon* (output wire), and the *axon terminals* (output terminals) which connect to other neurons via *synapses*.\n",
    "\n",
    "![The real neuron](../img/Neuron.svg)\n",
    "\n",
    "Information $x_i$ arriving from other neurons (or environmental sensors such as the retina) is received in the dendrites. In particular, that information is weighted by *synaptic weights* $w_i$ which determine how to respond to the inputs (e.g. activation or inhibition via $x_i w_i$). All this is aggregated in the nucleus $y = \\sum_i x_i w_i + b$, and this information is then sent for further processing in the axon $y$, typically after some nonlinear processing via $\\sigma(y)$. From there it either reaches its destination (e.g. a muscle) or is fed into another neuron via its dendrites.\n",
    "\n",
    "Brain *structures* vary significantly. Some look (to us) rather arbitrary whereas others have a regular structure. For example, the visual system of many insects is consistent across members of a species. The analysis of such structures has often inspired neuroscientists to propose new architectures, and in some cases, this has been successful. However, much research in artificial neural networks has little to do with any direct inspiration in neuroscience, just as although airplanes are *inspired* by birds, the study of orninthology hasn't been the primary driver of aeronautics innovaton in the last century. Equal amounts of inspiration these days comes from mathematics, statistics, and computer science.\n",
    "\n",
    "### Vectorization for Speed\n",
    "\n",
    "In model training or prediction, we often use vector calculations and process multiple observations at the same time. To illustrate why this matters, consider two methods of adding vectors. We begin by creating two 100000 dimensional ones first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2l \n",
    "import numpy as np\n",
    "import math\n",
    "from mxnet import nd\n",
    "import time\n",
    "\n",
    "n = 100000\n",
    "a = np.ones(n)\n",
    "b = np.ones(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will benchmark the running time frequently in this book, let's define a timer to do simply analysis of the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Save to the d2l package.\n",
    "class Timer(object):\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start the timer\"\"\"\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list\"\"\"\n",
    "        self.times.append(time.time() - self.start_time)\n",
    "        return self.times[-1]\n",
    "        \n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time\"\"\"\n",
    "        return sum(self.times)/len(self.times)\n",
    "    \n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time\"\"\"\n",
    "        return sum(self.times)\n",
    "        \n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumuated times\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can benchmark the workloads. One way to add vectors is to add them one coordinate at a time using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.05694 sec'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer = Timer()\n",
    "c = np.zeros(n)\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to add vectors is to add the vectors directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00053 sec'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the latter is vastly faster than the former. Vectorizing code is a good way of getting order of magnitude speedups. Likewise, as we saw above, it also greatly simplifies the mathematics and with it, it reduces the potential for errors in the notation.\n",
    "\n",
    "## The Normal Distribution and Squared Loss\n",
    "\n",
    "The following is optional and can be skipped but it will greatly help with understanding some of the design choices in building deep learning models. As we saw above, using the squared loss $l(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$ has many nice properties, such as having a particularly simple derivative $\\partial_{\\hat{y}} l(y, \\hat{y}) = (\\hat{y} - y)$. That is, the gradient is given by the difference between estimate and observation. You might reasonably point out that linear regression is a [classical](https://en.wikipedia.org/wiki/Regression_analysis#History) statistical model. Legendre first developed the method of least squares regression in 1805, which was shortly thereafter rediscovered by Gauss in 1809. To understand this a bit better, recall the normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right)$$\n",
    "\n",
    "Let's define the function to compute the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-7, 7, 0.01)\n",
    "\n",
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2) \n",
    "    return p * np.exp(- 0.5 / sigma**2 * (x - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a similar reason to create a `Timer` class, we define a `plot` function to draw multiple lines and set the figure properly, since we will visualize lines frequently later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to the d2l package.\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear', fmts=None,\n",
    "         figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot multiple lines\"\"\"\n",
    "    d2l.set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca()\n",
    "    if isinstance(X, nd.NDArray): X = X.asnumpy()\n",
    "    if isinstance(Y, nd.NDArray): Y = Y.asnumpy()\n",
    "    if not hasattr(X[0], \"__len__\"): X = [X]\n",
    "    if Y is None: X, Y = [[]]*len(X), X\n",
    "    if not hasattr(Y[0], \"__len__\"): Y = [Y]\n",
    "    if len(X) != len(Y): X = X * len(Y)\n",
    "    if not fmts: fmts = ['-']*len(X)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if isinstance(x, nd.NDArray): x = x.asnumpy()\n",
    "        if isinstance(y, nd.NDArray): y = y.asnumpy()\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "# Save to the d2l package.\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"A utility function to set matplotlib axes\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend: axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 302.08125 180.65625\" width=\"302.08125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 180.65625 \n",
       "L 302.08125 180.65625 \n",
       "L 302.08125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 143.1 \n",
       "L 294.88125 143.1 \n",
       "L 294.88125 7.2 \n",
       "L 43.78125 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 71.511736 143.1 \n",
       "L 71.511736 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mb81bcb5a2e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"71.511736\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −6 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.59375 35.5 \n",
       "L 73.1875 35.5 \n",
       "L 73.1875 27.203125 \n",
       "L 10.59375 27.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-8722\"/>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(64.140642 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 104.145435 143.1 \n",
       "L 104.145435 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.145435\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(96.774342 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 136.779135 143.1 \n",
       "L 136.779135 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.779135\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- −2 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(129.408041 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 169.412834 143.1 \n",
       "L 169.412834 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"169.412834\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(166.231584 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 202.046534 143.1 \n",
       "L 202.046534 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.046534\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(198.865284 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 234.680233 143.1 \n",
       "L 234.680233 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.680233\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(231.498983 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 267.313932 143.1 \n",
       "L 267.313932 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"267.313932\" xlink:href=\"#mb81bcb5a2e\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(264.132682 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- x -->\n",
       "     <defs>\n",
       "      <path d=\"M 54.890625 54.6875 \n",
       "L 35.109375 28.078125 \n",
       "L 55.90625 0 \n",
       "L 45.3125 0 \n",
       "L 29.390625 21.484375 \n",
       "L 13.484375 0 \n",
       "L 2.875 0 \n",
       "L 24.125 28.609375 \n",
       "L 4.6875 54.6875 \n",
       "L 15.28125 54.6875 \n",
       "L 29.78125 35.203125 \n",
       "L 44.28125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-120\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(166.371875 171.376563)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-120\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 43.78125 136.922727 \n",
       "L 294.88125 136.922727 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"md3b6a998d7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#md3b6a998d7\" y=\"136.922727\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 140.721946)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 43.78125 105.954474 \n",
       "L 294.88125 105.954474 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#md3b6a998d7\" y=\"105.954474\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.1 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 109.753693)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 43.78125 74.986221 \n",
       "L 294.88125 74.986221 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#md3b6a998d7\" y=\"74.986221\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(20.878125 78.78544)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 43.78125 44.017968 \n",
       "L 294.88125 44.017968 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#md3b6a998d7\" y=\"44.017968\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.3 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 47.817187)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path clip-path=\"url(#p7e34398d94)\" d=\"M 43.78125 13.049715 \n",
       "L 294.88125 13.049715 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#md3b6a998d7\" y=\"13.049715\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(20.878125 16.848934)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- p(x) -->\n",
       "     <defs>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 31 75.875 \n",
       "Q 24.46875 64.65625 21.28125 53.65625 \n",
       "Q 18.109375 42.671875 18.109375 31.390625 \n",
       "Q 18.109375 20.125 21.3125 9.0625 \n",
       "Q 24.515625 -2 31 -13.1875 \n",
       "L 23.1875 -13.1875 \n",
       "Q 15.875 -1.703125 12.234375 9.375 \n",
       "Q 8.59375 20.453125 8.59375 31.390625 \n",
       "Q 8.59375 42.28125 12.203125 53.3125 \n",
       "Q 15.828125 64.359375 23.1875 75.875 \n",
       "z\n",
       "\" id=\"DejaVuSans-40\"/>\n",
       "      <path d=\"M 8.015625 75.875 \n",
       "L 15.828125 75.875 \n",
       "Q 23.140625 64.359375 26.78125 53.3125 \n",
       "Q 30.421875 42.28125 30.421875 31.390625 \n",
       "Q 30.421875 20.453125 26.78125 9.375 \n",
       "Q 23.140625 -1.703125 15.828125 -13.1875 \n",
       "L 8.015625 -13.1875 \n",
       "Q 14.5 -2 17.703125 9.0625 \n",
       "Q 20.90625 20.125 20.90625 31.390625 \n",
       "Q 20.90625 42.671875 17.703125 53.65625 \n",
       "Q 14.5 64.65625 8.015625 75.875 \n",
       "z\n",
       "\" id=\"DejaVuSans-41\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 85.185156)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-40\"/>\n",
       "      <use x=\"102.490234\" xlink:href=\"#DejaVuSans-120\"/>\n",
       "      <use x=\"161.669922\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_25\">\n",
       "    <path clip-path=\"url(#p7e34398d94)\" d=\"M 55.194886 136.922727 \n",
       "L 108.224648 136.813535 \n",
       "L 113.609208 136.566288 \n",
       "L 117.198915 136.184417 \n",
       "L 119.97278 135.668952 \n",
       "L 122.257139 135.025186 \n",
       "L 124.215161 134.257847 \n",
       "L 126.010014 133.330368 \n",
       "L 127.804868 132.138334 \n",
       "L 129.436552 130.779445 \n",
       "L 131.068237 129.113085 \n",
       "L 132.699922 127.093512 \n",
       "L 134.331607 124.67477 \n",
       "L 135.963292 121.812692 \n",
       "L 137.594977 118.467288 \n",
       "L 139.226662 114.605495 \n",
       "L 141.021516 109.733818 \n",
       "L 142.816369 104.197078 \n",
       "L 144.774391 97.412516 \n",
       "L 146.895582 89.247632 \n",
       "L 149.506278 78.224523 \n",
       "L 153.259153 61.239305 \n",
       "L 157.501534 42.275258 \n",
       "L 159.785893 33.113056 \n",
       "L 161.580746 26.820515 \n",
       "L 163.049263 22.424523 \n",
       "L 164.354611 19.173268 \n",
       "L 165.49679 16.884633 \n",
       "L 166.475801 15.362585 \n",
       "L 167.454812 14.263605 \n",
       "L 168.270655 13.679589 \n",
       "L 169.086497 13.401979 \n",
       "L 169.739171 13.401979 \n",
       "L 170.391845 13.599455 \n",
       "L 171.207688 14.122466 \n",
       "L 172.02353 14.948577 \n",
       "L 173.002541 16.331186 \n",
       "L 173.981552 18.12656 \n",
       "L 175.123732 20.717347 \n",
       "L 176.42908 24.286979 \n",
       "L 177.897596 29.000684 \n",
       "L 179.69245 35.615359 \n",
       "L 181.81364 44.367165 \n",
       "L 184.913841 58.245076 \n",
       "L 190.46157 83.161144 \n",
       "L 192.909098 93.115047 \n",
       "L 195.030288 100.899673 \n",
       "L 196.98831 107.299489 \n",
       "L 198.783164 112.473243 \n",
       "L 200.578017 116.986089 \n",
       "L 202.209702 120.534567 \n",
       "L 203.841387 123.5855 \n",
       "L 205.473072 126.176453 \n",
       "L 207.104757 128.35023 \n",
       "L 208.736442 130.152334 \n",
       "L 210.368127 131.628806 \n",
       "L 211.999812 132.824481 \n",
       "L 213.794665 133.865803 \n",
       "L 215.752687 134.73293 \n",
       "L 217.873878 135.421686 \n",
       "L 220.321405 135.97206 \n",
       "L 223.258438 136.389278 \n",
       "L 227.011314 136.67952 \n",
       "L 232.395874 136.850878 \n",
       "L 243.001826 136.917995 \n",
       "L 283.467614 136.922727 \n",
       "L 283.467614 136.922727 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_26\">\n",
       "    <path clip-path=\"url(#p7e34398d94)\" d=\"M 55.194886 136.7876 \n",
       "L 65.800839 136.522951 \n",
       "L 73.143421 136.126434 \n",
       "L 79.017487 135.590287 \n",
       "L 83.912542 134.926494 \n",
       "L 88.318091 134.105229 \n",
       "L 92.234135 133.153577 \n",
       "L 95.823842 132.06316 \n",
       "L 99.250381 130.798749 \n",
       "L 102.51375 129.367709 \n",
       "L 105.77712 127.695008 \n",
       "L 109.04049 125.764111 \n",
       "L 112.30386 123.563438 \n",
       "L 115.56723 121.087897 \n",
       "L 118.993769 118.196043 \n",
       "L 122.583476 114.860886 \n",
       "L 126.49952 110.903144 \n",
       "L 131.068237 105.948686 \n",
       "L 138.247651 97.770807 \n",
       "L 144.611223 90.644946 \n",
       "L 148.527267 86.589675 \n",
       "L 151.790637 83.530672 \n",
       "L 154.564501 81.224436 \n",
       "L 157.175197 79.344214 \n",
       "L 159.459556 77.957408 \n",
       "L 161.743915 76.832365 \n",
       "L 163.865105 76.036198 \n",
       "L 165.823127 75.522597 \n",
       "L 167.781149 75.227168 \n",
       "L 169.739171 75.153089 \n",
       "L 171.697193 75.301158 \n",
       "L 173.655215 75.66978 \n",
       "L 175.613237 76.254995 \n",
       "L 177.734428 77.126088 \n",
       "L 179.855618 78.233161 \n",
       "L 182.139977 79.673625 \n",
       "L 184.587504 81.48006 \n",
       "L 187.361369 83.820867 \n",
       "L 190.46157 86.751111 \n",
       "L 194.051277 90.469337 \n",
       "L 198.783164 95.721751 \n",
       "L 211.673475 110.215093 \n",
       "L 215.752687 114.383393 \n",
       "L 219.505563 117.90542 \n",
       "L 222.932101 120.82526 \n",
       "L 226.195471 123.328273 \n",
       "L 229.458841 125.556324 \n",
       "L 232.722211 127.513772 \n",
       "L 235.985581 129.211619 \n",
       "L 239.248951 130.665971 \n",
       "L 242.675489 131.95258 \n",
       "L 246.265196 133.063568 \n",
       "L 250.18124 134.034481 \n",
       "L 254.423621 134.846714 \n",
       "L 259.155508 135.514669 \n",
       "L 264.540068 136.040363 \n",
       "L 270.903639 136.432354 \n",
       "L 279.225233 136.707949 \n",
       "L 283.467614 136.785216 \n",
       "L 283.467614 136.785216 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_27\">\n",
       "    <path clip-path=\"url(#p7e34398d94)\" d=\"M 55.194886 136.922727 \n",
       "L 157.175197 136.813535 \n",
       "L 162.559757 136.566288 \n",
       "L 166.149464 136.184417 \n",
       "L 168.923329 135.668952 \n",
       "L 171.207688 135.025186 \n",
       "L 173.16571 134.257847 \n",
       "L 174.960563 133.330368 \n",
       "L 176.755417 132.138334 \n",
       "L 178.387102 130.779445 \n",
       "L 180.018787 129.113085 \n",
       "L 181.650472 127.093512 \n",
       "L 183.282156 124.67477 \n",
       "L 184.913841 121.812692 \n",
       "L 186.545526 118.467288 \n",
       "L 188.177211 114.605495 \n",
       "L 189.972065 109.733818 \n",
       "L 191.766918 104.197078 \n",
       "L 193.72494 97.412516 \n",
       "L 195.846131 89.247632 \n",
       "L 198.456827 78.224523 \n",
       "L 202.209702 61.239305 \n",
       "L 206.452083 42.275258 \n",
       "L 208.736442 33.113056 \n",
       "L 210.531295 26.820515 \n",
       "L 211.999812 22.424523 \n",
       "L 213.30516 19.173268 \n",
       "L 214.447339 16.884633 \n",
       "L 215.42635 15.362585 \n",
       "L 216.405361 14.263605 \n",
       "L 217.221204 13.679589 \n",
       "L 218.037046 13.401979 \n",
       "L 218.68972 13.401979 \n",
       "L 219.342394 13.599455 \n",
       "L 220.158237 14.122466 \n",
       "L 220.974079 14.948577 \n",
       "L 221.95309 16.331186 \n",
       "L 222.932101 18.12656 \n",
       "L 224.074281 20.717347 \n",
       "L 225.379629 24.286979 \n",
       "L 226.848145 29.000684 \n",
       "L 228.642999 35.615359 \n",
       "L 230.764189 44.367165 \n",
       "L 233.864391 58.245076 \n",
       "L 239.412119 83.161144 \n",
       "L 241.859647 93.115047 \n",
       "L 243.980837 100.899673 \n",
       "L 245.938859 107.299489 \n",
       "L 247.733713 112.473243 \n",
       "L 249.528566 116.986089 \n",
       "L 251.160251 120.534567 \n",
       "L 252.791936 123.5855 \n",
       "L 254.423621 126.176453 \n",
       "L 256.055306 128.35023 \n",
       "L 257.686991 130.152334 \n",
       "L 259.318676 131.628806 \n",
       "L 260.950361 132.824481 \n",
       "L 262.745215 133.865803 \n",
       "L 264.703236 134.73293 \n",
       "L 266.824427 135.421686 \n",
       "L 269.271954 135.97206 \n",
       "L 272.208987 136.389278 \n",
       "L 275.961863 136.67952 \n",
       "L 281.346423 136.850878 \n",
       "L 283.467614 136.879593 \n",
       "L 283.467614 136.879593 \n",
       "\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 143.1 \n",
       "L 43.78125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 294.88125 143.1 \n",
       "L 294.88125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 143.1 \n",
       "L 294.88125 143.1 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 7.2 \n",
       "L 294.88125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 50.78125 59.234375 \n",
       "L 152.735938 59.234375 \n",
       "Q 154.735938 59.234375 154.735938 57.234375 \n",
       "L 154.735938 14.2 \n",
       "Q 154.735938 12.2 152.735938 12.2 \n",
       "L 50.78125 12.2 \n",
       "Q 48.78125 12.2 48.78125 14.2 \n",
       "L 48.78125 57.234375 \n",
       "Q 48.78125 59.234375 50.78125 59.234375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_28\">\n",
       "     <path d=\"M 52.78125 20.298438 \n",
       "L 72.78125 20.298438 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_29\"/>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- mean 0, var 1 -->\n",
       "     <defs>\n",
       "      <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "      <path id=\"DejaVuSans-32\"/>\n",
       "      <path d=\"M 11.71875 12.40625 \n",
       "L 22.015625 12.40625 \n",
       "L 22.015625 4 \n",
       "L 14.015625 -11.625 \n",
       "L 7.71875 -11.625 \n",
       "L 11.71875 4 \n",
       "z\n",
       "\" id=\"DejaVuSans-44\"/>\n",
       "      <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(80.78125 23.798438)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-109\"/>\n",
       "      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-44\"/>\n",
       "      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"635.9375\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_30\">\n",
       "     <path d=\"M 52.78125 34.976562 \n",
       "L 72.78125 34.976562 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_31\"/>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- mean 0, var 2 -->\n",
       "     <g transform=\"translate(80.78125 38.476562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-109\"/>\n",
       "      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-44\"/>\n",
       "      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"635.9375\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_32\">\n",
       "     <path d=\"M 52.78125 49.654688 \n",
       "L 72.78125 49.654688 \n",
       "\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_33\"/>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- mean 3, var 1 -->\n",
       "     <g transform=\"translate(80.78125 53.154688)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-109\"/>\n",
       "      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-51\"/>\n",
       "      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-44\"/>\n",
       "      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"635.9375\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p7e34398d94\">\n",
       "   <rect height=\"135.9\" width=\"251.1\" x=\"43.78125\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 324x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mean and variance pairs\n",
    "parameters = [(0,1), (0,2), (3,1)]\n",
    "plot(x, [normal(x, mu, sigma) for mu, sigma in parameters], \n",
    "     xlabel='x', ylabel='p(x)', figsize=(4.5, 2.5),\n",
    "     legend = ['mean %d, var %d'%(mu, sigma) for mu, sigma in parameters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the figure above, changing the mean shifts the function, increasing the variance makes it more spread-out with a lower peak. The key assumption in linear regression with least mean squares loss is that the observations actually arise from noisy observations, where noise is added to the data, e.g. as part of the observations process.\n",
    "\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "This allows us to write out the *likelihood* of seeing a particular $y$ for a given $\\mathbf{x}$ via\n",
    "\n",
    "$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right)$$\n",
    "\n",
    "A good way of finding the most likely values of $b$ and $\\mathbf{w}$ is to maximize the *likelihood* of the entire dataset\n",
    "\n",
    "$$p(Y|X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)})$$\n",
    "\n",
    "The notion of maximizing the likelihood of the data subject to the parameters is well known as the *Maximum Likelihood Principle* and its estimators are usually called *Maximum Likelihood Estimators* (MLE). Unfortunately, maximizing the product of many exponential functions is pretty awkward, both in terms of implementation and in terms of writing it out on paper. Instead, a much better way is to minimize the *Negative Log-Likelihood* $-\\log p(\\mathbf y|\\mathbf X)$. In the above case this works out to be\n",
    "\n",
    "$$-\\log p(\\mathbf y|\\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2$$\n",
    "\n",
    "A closer inspection reveals that for the purpose of minimizing $-\\log p(\\mathbf y|\\mathbf X)$ we can skip the first term since it doesn't depend on $\\mathbf{w}, b$ or even the data. The second term is identical to the objective we initially introduced, but for the multiplicative constant $\\frac{1}{\\sigma^2}$. Again, this can be skipped if we just want to get the most likely solution. It follows that maximum likelihood in a linear model with additive Gaussian noise is equivalent to linear regression with squared loss.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Key ingredients in a machine learning model are training data, a loss function, an optimization algorithm, and quite obviously, the model itself.\n",
    "* Vectorizing makes everything better (mostly math) and faster (mostly code).\n",
    "* Minimizing an objective function and performing maximum likelihood can mean the same thing.\n",
    "* Linear models are neural networks, too.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Assume that we have some data $x_1, \\ldots x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized.\n",
    "    * Find the optimal closed form solution.\n",
    "    * What does this mean in terms of the Normal distribution?\n",
    "1. Assume that we want to solve the optimization problem for linear regression with quadratic loss explicitly in closed form. To keep things simple, you can omit the bias $b$ from the problem.\n",
    "    * Rewrite the problem in matrix and vector notation (hint - treat all the data as a single matrix).\n",
    "    * Compute the gradient of the optimization problem with respect to $w$.\n",
    "    * Find the closed form solution by solving a matrix equation.\n",
    "    * When might this be better than using stochastic gradient descent (i.e. the incremental optimization approach that we discussed above)? When will this break (hint - what happens for high-dimensional $x$, what if many observations are very similar)?.\n",
    "1. Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$.\n",
    "    * Write out the negative log-likelihood of the data under the model $-\\log p(Y|X)$.\n",
    "    * Can you find a closed form solution?\n",
    "    * Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?\n",
    "\n",
    "## Scan the QR Code to [Discuss](https://discuss.mxnet.io/t/2331)\n",
    "\n",
    "![](../img/qr_linear-regression.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}